# ============================================================================
# GitHub AI Agent v2.0 Configuration Template
# Copy this to .env and fill in your values
# ============================================================================

# ============================================================================
# LLM Provider Selection (NEW!)
# ============================================================================

# Choose your LLM provider: ollama, groq, or openai
# - ollama: Local, free, requires GPU (default)
# - groq: Cloud, fast, free tier available
# - openai: Cloud, most powerful, requires API key
LLM_PROVIDER=ollama

# ============================================================================
# API Keys (based on provider choice)
# ============================================================================

# GROQ API Key (if LLM_PROVIDER=groq)
# Get free key: https://console.groq.com/keys
GROQ_API_KEY=gsk_your_groq_api_key_here

# OpenAI API Key (if LLM_PROVIDER=openai)
# Get key: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_openai_api_key_here

# ============================================================================
# Required Configuration
# ============================================================================

# GitHub Personal Access Token
# Create at: https://github.com/settings/tokens
GITHUB_TOKEN=ghp_your_github_token_here

# Target Repository
# Format: username/repository
REPO_FULL_NAME=your_username/your_repository

# ============================================================================
# Optional Configuration
# ============================================================================

# HuggingFace Token (Fallback LLM)
HUGGINGFACE_TOKEN=hf_your_huggingface_token_here

# Environment
ENVIRONMENT=development

# Debug Mode (shows detailed logs)
DEBUG=false

# ============================================================================
# LLM Configuration
# ============================================================================

# GROQ Model
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_TIMEOUT=30
GROQ_MAX_TOKENS=2048
GROQ_TEMPERATURE=0.7

# HuggingFace Model
HUGGINGFACE_MODEL=meta-llama/Llama-2-7b-chat-hf

# ============================================================================
# Web Server Configuration
# ============================================================================

CHAT_HOST=0.0.0.0
CHAT_PORT=5000

# ============================================================================
# Execution Configuration
# ============================================================================

ENABLE_CODE_EXECUTION=true
ENABLE_SHELL_EXECUTION=false
ENABLE_GIT_OPERATIONS=true
CODE_EXECUTION_TIMEOUT=10
SHELL_EXECUTION_TIMEOUT=30

# ============================================================================
# Logging
# ============================================================================

LOG_LEVEL=INFO

# ============================================================================
# Security
# ============================================================================

MAX_FILE_SIZE=10485760
MAX_PROMPT_LENGTH=4000
MAX_ISSUE_BODY_LENGTH=2000

# ============================================================================
# Mode
# ============================================================================

MODE=cloud
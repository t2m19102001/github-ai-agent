#!/usr/bin/env python3
"""
GitHub AI Agent - Cloud LLM Only
T·ª± ƒë·ªông x·ª≠ l√Ω GitHub Issues s·ª≠ d·ª•ng GROQ & HuggingFace (Cloud LLM)
"""

import os
import json
import requests
import logging
import re
from dotenv import load_dotenv
from github import Github
from typing import Optional
import time

load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.DEBUG if os.getenv("DEBUG", "false").lower() == "true" else logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
REPO_FULL_NAME = os.getenv("REPO_FULL_NAME")
MODE = os.getenv("MODE", "cloud")  # Only 'cloud' mode
DEBUG = os.getenv("DEBUG", "false").lower() == "true"

# Cloud Mode (GROQ) - Primary
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = "llama-3.3-70b-versatile"  # GROQ free tier model (updated)

# Cloud Mode (HuggingFace) - Fallback
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
HUGGINGFACE_MODEL = os.getenv("HUGGINGFACE_MODEL", "meta-llama/Llama-2-7b-chat-hf")

# Validation constraints
MAX_PROMPT_LENGTH = 4000
MAX_ISSUE_BODY_LENGTH = 2000

def sanitize_text(text: str, max_length: int = MAX_ISSUE_BODY_LENGTH) -> str:
    """
    Sanitize input text to prevent injection attacks
    """
    if not text:
        return ""
    
    # Truncate to max length
    text = text[:max_length]
    
    # Remove potentially dangerous characters (but keep markdown)
    # Keep alphanumeric, spaces, common punctuation, newlines, and markdown
    safe_chars = re.compile(r'[^a-zA-Z0-9\s\-_.,:;!?()\[\]{}*`#\n\r/\\@]')
    text = safe_chars.sub('', text)
    
    return text.strip()

def validate_prompt(prompt: str) -> bool:
    """
    Validate prompt before sending to LLM
    """
    if not prompt or len(prompt) == 0:
        logger.warning("Empty prompt provided")
        return False
    
    if len(prompt) > MAX_PROMPT_LENGTH:
        logger.warning(f"Prompt too long: {len(prompt)} > {MAX_PROMPT_LENGTH}")
        return False
    
    return True

class GitHubAIAgent:
    def __init__(self):
        """Initialize the AI Agent"""
        self.gh = Github(GITHUB_TOKEN)
        self.repo = self.gh.get_repo(REPO_FULL_NAME)
        self.mode = "cloud"  # Only cloud mode supported
        
    def get_response_huggingface(self, prompt: str) -> Optional[str]:
        """
        Get response t·ª´ HuggingFace Inference API (Cloud)
        """
        if not validate_prompt(prompt):
            return None
        
        try:
            if DEBUG:
                print(f"üì° Calling HuggingFace API...")
            
            headers = {"Authorization": f"Bearer {HUGGINGFACE_TOKEN}"}
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_length": 500,
                    "temperature": 0.7,
                }
            }
            
            response = requests.post(
                f"https://api-inference.huggingface.co/models/{HUGGINGFACE_MODEL}",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            
            if isinstance(result, list) and len(result) > 0:
                return result[0].get("generated_text", "").strip()
            return None
        except Exception as e:
            if DEBUG:
                print(f"‚ùå HuggingFace error: {e}")
            return None
    
    def get_response_groq(self, prompt: str) -> Optional[str]:
        """
        Get response t·ª´ GROQ API (Cloud - Fast inference)
        """
        if not validate_prompt(prompt):
            return None
        
        try:
            if DEBUG:
                logger.info(f"üì° Calling GROQ API with model {GROQ_MODEL}...")
            
            headers = {
                "Authorization": f"Bearer {GROQ_API_KEY}",
                "Content-Type": "application/json"
            }
            payload = {
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "model": GROQ_MODEL,
                "temperature": 0.7,
                "max_tokens": 1024,
            }
            
            response = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0].get("message", {}).get("content", "").strip()
            return None
        except requests.exceptions.Timeout:
            if DEBUG:
                logger.error(f"‚ùå GROQ timeout")
            return None
        except requests.exceptions.ConnectionError:
            if DEBUG:
                logger.error(f"‚ùå GROQ connection error")
            return None
        except Exception as e:
            if DEBUG:
                logger.error(f"‚ùå GROQ error: {e}")
            return None
    
    def get_llm_response(self, prompt: str) -> Optional[str]:
        """
        Get response t·ª´ LLM (Cloud Mode)
        Th·ª≠ GROQ ‚Üí HuggingFace theo priority
        """
        if DEBUG:
            logger.info("üîÑ Mode: Cloud")
        
        # Try GROQ first (faster)
        if GROQ_API_KEY:
            if DEBUG:
                logger.info("üîÑ Trying GROQ API...")
            response = self.get_response_groq(prompt)
            if response:
                return response
            if DEBUG:
                logger.warning("‚ö†Ô∏è GROQ failed, trying HuggingFace...")
        
        # Try HuggingFace as fallback
        if HUGGINGFACE_TOKEN:
            if DEBUG:
                logger.info("üîÑ Trying HuggingFace API...")
            response = self.get_response_huggingface(prompt)
            if response:
                return response
        
        return None
    
    def get_issue_context(self, issue):
        """
        L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ issue
        """
        issue_body = sanitize_text(issue.body if issue.body else "No description provided", MAX_ISSUE_BODY_LENGTH)
        labels = ', '.join([label.name for label in issue.labels]) if issue.labels else "None"
        
        context = f"""
GitHub Issue Analysis
======================
Title: {issue.title}
Number: #{issue.number}
State: {issue.state}
Author: @{issue.user.login}
Created: {issue.created_at}

Body:
{issue_body}

Labels: {labels}
"""
        return context
    
    def generate_analysis(self, issue) -> Optional[str]:
        """
        Generate AI analysis cho issue
        """
        context = self.get_issue_context(issue)
        
        prompt = f"""{context}

B·∫°n l√† m·ªôt senior software engineer v·ªõi 10 nƒÉm kinh nghi·ªám. H√£y ph√¢n t√≠ch GitHub Issue n√†y m·ªôt c√°ch chi ti·∫øt v√† chuy√™n nghi·ªáp.

**Y√™u c·∫ßu ph√¢n t√≠ch:**

1. **T√≥m t·∫Øt v·∫•n ƒë·ªÅ** (2-3 c√¢u)
   - N√™u r√µ v·∫•n ƒë·ªÅ ch√≠nh, t√°c ƒë·ªông v√† ƒë·ªô ∆∞u ti√™n

2. **Root cause analysis** (2-3 ƒëo·∫°n)
   - Ph√¢n t√≠ch nguy√™n nh√¢n g·ªëc r·ªÖ
   - C√°c y·∫øu t·ªë li√™n quan

3. **Gi·∫£i ph√°p ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t** (2-3 ph∆∞∆°ng √°n)
   - Li·ªát k√™ c√°c c√°ch ti·∫øp c·∫≠n kh√°c nhau
   - ƒê√°nh gi√° ∆∞u/nh∆∞·ª£c ƒëi·ªÉm c·ªßa m·ªói gi·∫£i ph√°p

4. **Th·ª±c hi·ªán chi ti·∫øt** 
   - C√°c b∆∞·ªõc c·ª• th·ªÉ ƒë·ªÉ gi·∫£i quy·∫øt
   - ∆Ø·ªõc t√≠nh ƒë·ªô ph·ª©c t·∫°p (Easy/Medium/Hard)

5. **Code example** (n·∫øu c√≥ li√™n quan)
   - Cung c·∫•p code snippet minh h·ªça
   - S·ª≠ d·ª•ng markdown code blocks

6. **Testing approach**
   - C√°ch test solution
   - Test cases c·∫ßn ki·ªÉm tra

7. **R·ªßi ro ti·ªÅm ·∫©n**
   - Nh·ªØng v·∫•n ƒë·ªÅ c√≥ th·ªÉ g·∫∑p
   - C√°ch gi·∫£m thi·ªÉu r·ªßi ro

8. **T√†i li·ªáu tham kh·∫£o**
   - Links ho·∫∑c tips h·ªØu √≠ch (n·∫øu c√≥)

**L∆∞u √Ω:**
- Tr·∫£ l·ªùi b·∫±ng **Ti·∫øng Vi·ªát**
- Chi ti·∫øt v√† chuy√™n nghi·ªáp
- T·∫≠p trung v√†o t√≠nh kh·∫£ thi
- D·ªÖ hi·ªÉu cho developer c·ªßa d·ª± √°n
"""
        
        # Validate prompt
        if not validate_prompt(prompt):
            logger.error(f"Invalid prompt for issue #{issue.number}")
            return None
        
        if DEBUG:
            logger.info(f"üß† Generating AI analysis for issue #{issue.number}...")
        
        response = self.get_llm_response(prompt)
        return response
    
    def process_issue(self, issue_number: int):
        """
        X·ª≠ l√Ω m·ªôt GitHub Issue
        """
        print(f"\n{'='*60}")
        print(f"üîç Processing Issue #{issue_number}...")
        print(f"{'='*60}")
        
        try:
            issue = self.repo.get_issue(issue_number)
            
            print(f"üìå Title: {issue.title}")
            print(f"üë§ Author: @{issue.user.login}")
            print(f"üìù Status: {issue.state}")
            
            # Check if already commented
            comments = list(issue.get_comments())
            for comment in comments:
                if "AI Agent" in comment.body or "ü§ñ" in comment.body:
                    print("‚è≠Ô∏è  Already analyzed by AI Agent, skipping...")
                    logger.info(f"Issue #{issue_number} already analyzed, skipping")
                    return
            
            # Generate analysis
            analysis = self.generate_analysis(issue)
            
            if analysis:
                print("‚úÖ AI Analysis generated successfully")
                
                # Comment on issue
                comment_body = f"""## ü§ñ AI Agent Analysis

{analysis}

---
*Generated by GitHub AI Agent (Hybrid Local & Cloud LLM)*
*Last updated: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
                issue.create_comment(comment_body)
                print("‚úÖ Comment posted on issue")
                logger.info(f"Issue #{issue_number} analysis completed and commented")
            else:
                print("‚ùå Failed to generate analysis")
                logger.warning(f"Failed to generate analysis for issue #{issue_number}")
                # Post error comment
                issue.create_comment("""
## ‚ùå AI Agent Error

Sorry, I couldn't generate analysis at this moment.
Please try again later or check the configuration.

---
*GitHub AI Agent*
""")
        
        except Exception as e:
            print(f"‚ùå Error processing issue: {e}")
            logger.error(f"Error processing issue #{issue_number}: {e}", exc_info=True)
    
    def process_open_issues(self, limit: int = 5):
        """
        X·ª≠ l√Ω t·∫•t c·∫£ c√°c open issues
        """
        print(f"\n{'='*60}")
        print(f"üìä Fetching open issues from {self.repo.full_name}...")
        print(f"{'='*60}")
        
        try:
            issues = self.repo.get_issues(state="open")
            
            count = 0
            for issue in issues:
                if count >= limit:
                    break
                if not issue.pull_request:  # Skip PRs
                    self.process_issue(issue.number)
                    count += 1
                    time.sleep(2)  # Rate limiting
            
            print(f"\n‚úÖ Processed {count} issues")
        
        except Exception as e:
            print(f"‚ùå Error fetching issues: {e}")
    
    def test_connection(self) -> bool:
        """
        Test connections to GitHub & LLM
        """
        print("\n" + "="*60)
        print("üß™ Testing Connections...")
        print("="*60)
        
        # Test GitHub
        try:
            repo = self.repo
            print(f"‚úÖ GitHub: Connected to {repo.full_name}")
        except Exception as e:
            print(f"‚ùå GitHub: {e}")
            logger.error(f"GitHub connection failed: {e}")
            return False
        
        # Test GROQ
        test_prompt = "Say 'Hello' in one word"
        
        if GROQ_API_KEY:
            print(f"\nüì° Testing GROQ API...")
            response = self.get_response_groq(test_prompt)
            if response:
                print(f"‚úÖ GROQ: Connected")
            else:
                print(f"‚ö†Ô∏è  GROQ: Connection issue")
                if not HUGGINGFACE_TOKEN:
                    logger.error("GROQ failed and no HuggingFace token")
                    return False
        else:
            print(f"‚ö†Ô∏è  GROQ_API_KEY: Not provided")
        
        # Test HuggingFace fallback
        if HUGGINGFACE_TOKEN:
            print(f"\nüì° Testing HuggingFace API...")
            response = self.get_response_huggingface(test_prompt)
            if response:
                print(f"‚úÖ HuggingFace: Connected")
            else:
                print(f"‚ö†Ô∏è  HuggingFace: Connection issue")
        else:
            print(f"‚ö†Ô∏è  HUGGINGFACE_TOKEN: Not provided (recommended to add as fallback)")
        
        if not GROQ_API_KEY and not HUGGINGFACE_TOKEN:
            print(f"\n‚ùå No LLM API keys configured!")
            return False
        
        return True

def main():
    """Main entry point"""
    print("\n" + "="*60)
    print("üöÄ GitHub AI Agent Starting...")
    print("="*60)
    print(f"Mode: Cloud (GROQ + HuggingFace)")
    print(f"Repository: {REPO_FULL_NAME}")
    
    if not GITHUB_TOKEN:
        print("‚ùå GITHUB_TOKEN not found in environment variables")
        logger.error("GITHUB_TOKEN not configured")
        return
    
    if not REPO_FULL_NAME:
        print("‚ùå REPO_FULL_NAME not found in environment variables")
        logger.error("REPO_FULL_NAME not configured")
        return
    
    if not GROQ_API_KEY and not HUGGINGFACE_TOKEN:
        print("‚ùå At least one LLM API key required (GROQ_API_KEY or HUGGINGFACE_TOKEN)")
        logger.error("No LLM API keys configured")
        return
    
    try:
        agent = GitHubAIAgent()
        
        # Test connections
        if not agent.test_connection():
            print("\n‚ùå Connection test failed!")
            logger.error("Connection test failed")
            return
        
        # Process issues
        print("\n" + "="*60)
        agent.process_open_issues(limit=5)
        
        print("\n" + "="*60)
        print("‚úÖ GitHub AI Agent completed!")
        print("="*60)
        logger.info("GitHub AI Agent completed successfully")
    
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        logger.error(f"Fatal error in main: {e}", exc_info=True)

if __name__ == "__main__":
    main()